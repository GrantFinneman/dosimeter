{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction notebook\n",
    "\n",
    "This notebook will be used to extract the data from the ohio runs and potentially the data generated by geant\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ohio Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from my_classes import load_data_ohio\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates a list of file paths of the original ohio data\n",
    "- Does not add the file to the list if help or 22 is in the name since help.txt is not a data file and run 22 is garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path = 'ohio_data/runs/'\n",
    "run_paths = [os.path.join(run_path, file) for file in os.listdir(run_path)\n",
    "             if all(('help' not in file, '22' not in file, file.endswith('.txt')))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates a dictionary holding each file and the energy deposition of each channel\n",
    "\n",
    "- Some values are negative which doesn't make sense in this context since they are measureing light exposure through coulombs. Because of the random negative values I translate everything up by the magnitude of the most negative number (subtract the most negative number from everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_edeps = {}\n",
    "for file_path in run_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    df = pd.read_csv(file_path, delimiter='\\t', skiprows=19)\n",
    "    overall_min = df[5:-2].min().min() # Needs to be done twice dince the first one gives [1, 64] array of minimums of each column\n",
    "    \n",
    "    channel_edep = {}\n",
    "    for col in df.columns[5:-2]:\n",
    "        channel_edep[col] = sum(df[col] + abs(overall_min))\n",
    "    \n",
    "    file_edeps[file_name] = channel_edep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saves the channel and the corresponding energy deposition in a file with the same name as the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in file_edeps:\n",
    "    with open(f'ohio_data/extracted_data/{file}', 'w') as fhandle:\n",
    "        print(f'''This file was gnenerated on {str(datetime.today())} by Grant Finneman. This contains the data colleted by\\nAlex in Ohio. The only processing that has been done so far is summing the edep of each channel.''', file=fhandle)\n",
    "        print('Channel edep', file=fhandle)\n",
    "        \n",
    "        for channel, edep in file_edeps[file].items():\n",
    "            print(f\"{channel.replace(' ',''):5s} {edep:.5f}\", file=fhandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geant Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating list of cube and bar files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'geant_data/dosimeterDataMMRotated_Completed_1_19_2020/'\n",
    "filenames = sorted([os.path.join(data_dir, file) for file in os.listdir(data_dir)\n",
    "            if all(('Training' not in file, 'All' not in file, file.endswith('.txt')))])\n",
    "\n",
    "cube_filenames = [path for path in filenames if 'Cube' in path]\n",
    "bar_filenames = [path for path in filenames if 'Bars' in path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generates dictionaries of bar files with edep array, cube files and edep array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_edeps = {}\n",
    "for path in cube_filenames:\n",
    "    filename = os.path.basename(path)\n",
    "    df = pd.read_csv(path, delim_whitespace=True, names=['x_dim', 'y_dim', 'z_dim', 'edep'])\n",
    "    cube_edeps[filename] = list(df['edep'])\n",
    "    \n",
    "bar_edeps = {}\n",
    "for path in bar_filenames:\n",
    "    filename = os.path.basename(path)\n",
    "    df = pd.read_csv(path, delim_whitespace=True, names=['x_dim', 'y_dim', 'z_dim', 'edep'])\n",
    "    bar_edeps[filename] = list(df['edep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Writes the bar edep and cube edep to their extraction files\n",
    "- Not much processing is required here since there are no weird values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_cubes = 'geant_data/extracted_data/cubes/'\n",
    "\n",
    "for filename, data in cube_edeps.items():\n",
    "    new_name = f'{filename.rstrip(\".txt\")}_extracted.txt'\n",
    "    with open(os.path.join(extracted_cubes, new_name), 'w') as file:\n",
    "        print('cubeID edep', file=file)\n",
    "        for ID, edep in enumerate(data):\n",
    "            print(f'{ID} {edep:.5f}', file=file)\n",
    "            \n",
    "extracted_bars = 'geant_data/extracted_data/bars/'\n",
    "\n",
    "for filename, data in bar_edeps.items():\n",
    "    new_name = f'{filename.rstrip(\".txt\")}_extracted.txt'\n",
    "    with open(os.path.join(extracted_bars, new_name), 'w') as file:\n",
    "        print('barID edep', file=file)\n",
    "        for ID, edep in enumerate(data):\n",
    "            print(f'{ID} {edep:.5f}', file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gate Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_dir = 'gate_simulations/bar_simulation/8_8detector/output/'\n",
    "paths = [os.path.join(hit_dir, file) for file in os.listdir(hit_dir)\n",
    "        if all(('Hits' in file, True))]\n",
    "\n",
    "# generates the correct names for the columns I want\n",
    "names = [f'col{num}' for num in range(1, 24)]\n",
    "names[7] = 'id'\n",
    "names[8] = 'edep'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv(paths[0], delim_whitespace=True, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gate_simulations/bar_simulation/8_8detector/output/W05__H05__A000_bars_Hits.dat\n",
      "gate_simulations/bar_simulation/8_8detector/output/W05__H05__A020_bars_Hits.dat\n"
     ]
    }
   ],
   "source": [
    "bar_file_dict = {}\n",
    "for path in paths:\n",
    "    print(path)\n",
    "    f_name = os.path.basename(path)\n",
    "    \n",
    "    # Initializes the dictionary with keys from 0-63 and values of 0\n",
    "    file_edep = dict.fromkeys(list(range(64)), 0)\n",
    "    \n",
    "    # Loads the data from the current Hits file\n",
    "    df = pd.read_csv(path, delim_whitespace=True, names=names)\n",
    "    \n",
    "    for id_num, edep in zip(df['id'], df['edep']):\n",
    "        file_edep[id_num] += edep\n",
    "\n",
    "    # Assigns the edep dictionary to the overall dictionary of files\n",
    "    bar_file_dict[f_name] = file_edep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dosimeter]",
   "language": "python",
   "name": "conda-env-dosimeter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
